{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adf823db-6c63-4b3a-aff6-aadc169c17d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will store data in:\nLanding Orders folder:    dbfs:/FileStore/tables/dlt/landing/orders\nLanding Customers folder: dbfs:/FileStore/tables/dlt/landing/customers\nSilver Delta folder:      dbfs:/tmp/delta/sil_orders\nSQL Table name:           sil_orders_tbl\n"
     ]
    }
   ],
   "source": [
    " #STEP 0 — Setup\n",
    "# These variables store the paths for each stage of the pipeline\n",
    "\n",
    "# Landing folders: raw files exactly as they arrive\n",
    "LANDING_ORDERS    = \"dbfs:/FileStore/tables/dlt/landing/orders\"\n",
    "LANDING_CUSTOMERS = \"dbfs:/FileStore/tables/dlt/landing/customers\"\n",
    "\n",
    "# Silver folder: cleaned data in Delta format\n",
    "DELTA_SILVER_PATH = \"dbfs:/tmp/delta/sil_orders\"\n",
    "\n",
    "# SQL table name pointing to the silver Delta folder\n",
    "DELTA_TABLE_NAME  = \"sil_orders_tbl\"\n",
    "\n",
    "print(\"We will store data in:\")\n",
    "print(f\"Landing Orders folder:    {LANDING_ORDERS}\")\n",
    "print(f\"Landing Customers folder: {LANDING_CUSTOMERS}\")\n",
    "print(f\"Silver Delta folder:      {DELTA_SILVER_PATH}\")\n",
    "print(f\"SQL Table name:           {DELTA_TABLE_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e39815fb-1dbd-482d-82f2-77e5fd585dc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Seeding inline data to landing (JSON) ...\n✅ Seeded landing JSON:\n  /FileStore/tables/landing/orders_json\n  /FileStore/tables/landing/customers_json\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# Paths where JSON files will be saved\n",
    "LANDING_ORDERS = \"/FileStore/tables/landing/orders_json\"\n",
    "LANDING_CUSTOMERS = \"/FileStore/tables/landing/customers_json\"\n",
    "\n",
    "print(\"STEP 1: Seeding inline data to landing (JSON) ...\")\n",
    "\n",
    "# Orders data\n",
    "orders_rows = [\n",
    "    (1, \"C001\", \"2025-08-08 09:00:00\", 12000, \"placed\"),\n",
    "    (2, \"C002\", \"2025-08-08 09:05:00\",  4500, \"placed\"),\n",
    "    (3, \"C001\", \"2025-08-08 09:10:00\", 22000, \"cancelled\"),\n",
    "    (4, \"C003\", \"2025-08-08 09:15:00\",   800, \"placed\")\n",
    "]\n",
    "\n",
    "# Customers data\n",
    "customers_rows = [\n",
    "    (\"C001\", \"Ananya\", \"Bengaluru\"),\n",
    "    (\"C002\", \"Rahul\",  \"Hyderabad\"),\n",
    "    (\"C003\", \"Meera\",  \"Pune\")\n",
    "]\n",
    "\n",
    "# Orders schema\n",
    "orders_schema = T.StructType([\n",
    "    T.StructField(\"order_id\",    T.IntegerType()),\n",
    "    T.StructField(\"customer_id\", T.StringType()),\n",
    "    T.StructField(\"order_ts\",    T.StringType()),\n",
    "    T.StructField(\"amount\",      T.IntegerType()),\n",
    "    T.StructField(\"status\",      T.StringType())\n",
    "])\n",
    "\n",
    "# Customers schema\n",
    "cust_schema = T.StructType([\n",
    "    T.StructField(\"customer_id\", T.StringType()),\n",
    "    T.StructField(\"name\",        T.StringType()),\n",
    "    T.StructField(\"city\",        T.StringType())\n",
    "])\n",
    "\n",
    "# Create DataFrames\n",
    "orders_df = (spark.createDataFrame(orders_rows, orders_schema)\n",
    "             .withColumn(\"order_ts\", F.to_timestamp(\"order_ts\")))\n",
    "customers_df = spark.createDataFrame(customers_rows, cust_schema)\n",
    "\n",
    "# Write JSON files\n",
    "orders_df.write.mode(\"overwrite\").json(LANDING_ORDERS)\n",
    "customers_df.write.mode(\"overwrite\").json(LANDING_CUSTOMERS)\n",
    "\n",
    "print(\"✅ Seeded landing JSON:\")\n",
    "print(f\"  {LANDING_ORDERS}\")\n",
    "print(f\"  {LANDING_CUSTOMERS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d5ac6b7-4f68-4103-9825-5d971449e3ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: BRONZE - Reading raw landing data (no transformations)\nBronze Orders - schema & sample:\nroot\n |-- amount: long (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- order_id: long (nullable = true)\n |-- order_ts: string (nullable = true)\n |-- status: string (nullable = true)\n\n+------+-----------+--------+------------------------+---------+\n|amount|customer_id|order_id|order_ts                |status   |\n+------+-----------+--------+------------------------+---------+\n|22000 |C001       |3       |2025-08-08T09:10:00.000Z|cancelled|\n|12000 |C001       |1       |2025-08-08T09:00:00.000Z|placed   |\n|4500  |C002       |2       |2025-08-08T09:05:00.000Z|placed   |\n|800   |C003       |4       |2025-08-08T09:15:00.000Z|placed   |\n+------+-----------+--------+------------------------+---------+\n\nBronze Customers - schema & sample:\nroot\n |-- city: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- name: string (nullable = true)\n\n+---------+-----------+------+\n|city     |customer_id|name  |\n+---------+-----------+------+\n|Bengaluru|C001       |Ananya|\n|Hyderabad|C002       |Rahul |\n|Pune     |C003       |Meera |\n+---------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 2: BRONZE - Reading raw landing data (no transformations)\")\n",
    "\n",
    "bron_orders = spark.read.json(LANDING_ORDERS)\n",
    "bron_customers = spark.read.json(LANDING_CUSTOMERS)\n",
    "\n",
    "print(\"Bronze Orders - schema & sample:\")\n",
    "bron_orders.printSchema()\n",
    "bron_orders.show(truncate=False)\n",
    "\n",
    "print(\"Bronze Customers - schema & sample:\")\n",
    "bron_customers.printSchema()\n",
    "bron_customers.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1e3a642-c0f1-4f33-8bac-e25d87ff2555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: SILVER - Cleaning data & writing to Delta\nWrote silver orders to Delta path:\ndbfs:/tmp/delta/sil_orders\nReading back from Delta to verify:\n+--------+-----------+------------------------+------+---------+\n|order_id|customer_id|order_ts                |amount|status   |\n+--------+-----------+------------------------+------+---------+\n|3       |C001       |2025-08-08T09:10:00.000Z|22000 |cancelled|\n|1       |C001       |2025-08-08T09:00:00.000Z|12000 |placed   |\n|4       |C003       |2025-08-08T09:15:00.000Z|800   |placed   |\n|2       |C002       |2025-08-08T09:05:00.000Z|4500  |placed   |\n+--------+-----------+------------------------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 3: SILVER - Cleaning data & writing to Delta\")\n",
    "\n",
    "sil_orders = (\n",
    "    bron_orders\n",
    "    .select(\"order_id\", \"customer_id\", \"order_ts\", \"amount\", \"status\")\n",
    "    .filter(\"order_id IS NOT NULL AND amount >= 0\")\n",
    ")\n",
    "\n",
    "# Write to Delta (overwrite for a clean slate)\n",
    "sil_orders.write.format(\"delta\").mode(\"overwrite\").save(DELTA_SILVER_PATH)\n",
    "\n",
    "print(\"Wrote silver orders to Delta path:\")\n",
    "print(f\"{DELTA_SILVER_PATH}\")\n",
    "\n",
    "print(\"Reading back from Delta to verify:\")\n",
    "spark.read.format(\"delta\").load(DELTA_SILVER_PATH).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32b79df8-0474-4c30-9062-67ad9ecca185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5: GOLD - Enrich orders by joining with customers\nGold Enriched - sample:\n+-----------+--------+------------------------+------+---------+---------+------+\n|customer_id|order_id|order_ts                |amount|status   |city     |name  |\n+-----------+--------+------------------------+------+---------+---------+------+\n|C001       |3       |2025-08-08T09:10:00.000Z|22000 |cancelled|Bengaluru|Ananya|\n|C001       |1       |2025-08-08T09:00:00.000Z|12000 |placed   |Bengaluru|Ananya|\n|C003       |4       |2025-08-08T09:15:00.000Z|800   |placed   |Pune     |Meera |\n|C002       |2       |2025-08-08T09:05:00.000Z|4500  |placed   |Hyderabad|Rahul |\n+-----------+--------+------------------------+------+---------+---------+------+\n\nGold Daily Revenue - sample:\n+----------+-------------+\n|order_date|total_revenue|\n+----------+-------------+\n|2025-08-08|        39300|\n+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"STEP 5: GOLD - Enrich orders by joining with customers\")\n",
    "\n",
    "# Read silver orders\n",
    "sil_orders_df = spark.read.format(\"delta\").load(DELTA_SILVER_PATH)\n",
    "\n",
    "# Enrich with customer info\n",
    "gold_enriched = (\n",
    "    sil_orders_df.alias(\"o\")\n",
    "    .join(bron_customers.alias(\"c\"), on=\"customer_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "print(\"Gold Enriched - sample:\")\n",
    "gold_enriched.show(truncate=False)\n",
    "\n",
    "#  Create daily revenue DataFrame\n",
    "gold_daily = (\n",
    "    gold_enriched\n",
    "    .withColumn(\"order_date\", F.to_date(\"order_ts\"))\n",
    "    .groupBy(\"order_date\")\n",
    "    .agg(F.sum(\"amount\").alias(\"total_revenue\"))\n",
    "    .orderBy(\"order_date\")\n",
    ")\n",
    "\n",
    "print(\"Gold Daily Revenue - sample:\")\n",
    "gold_daily.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "471ac5b7-ada4-4218-a210-b13f9bf3e3cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a0892d-dc75-4d7e-93f7-b2305a2632a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load Delta table\n",
    "delta_table = DeltaTable.forPath(spark, DELTA_SILVER_PATH)\n",
    "\n",
    "# Update all cancelled orders for C001 to 'returned'\n",
    "delta_table.update(\n",
    "    condition=\"customer_id = 'C001' AND status = 'cancelled'\",\n",
    "    set={\"status\": \"'returned'\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a277926b-6974-4d77-b87b-8862ebe2d177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta_table.delete(\"amount < 1000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59827260-6c16-43eb-88e7-9aa84e17f54f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_orders = [\n",
    "    (5, \"C002\", \"2025-08-09 10:00:00\", 5000, \"placed\"),\n",
    "    (3, \"C001\", \"2025-08-08 09:10:00\", 23000, \"placed\")  # same order_id as before\n",
    "]\n",
    "\n",
    "new_schema = [\"order_id\", \"customer_id\", \"order_ts\", \"amount\", \"status\"]\n",
    "new_df = spark.createDataFrame(new_orders, new_schema)\n",
    "new_df = new_df.withColumn(\"order_ts\", F.to_timestamp(\"order_ts\"))\n",
    "\n",
    "delta_table.alias(\"tgt\").merge(\n",
    "    new_df.alias(\"src\"),\n",
    "    \"tgt.order_id = src.order_id\"\n",
    ").whenMatchedUpdateAll(\n",
    ").whenNotMatchedInsertAll(\n",
    ").execute()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "August 11 practise",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}