{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b2bb0d44-6615-4585-97eb-27919c57f382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pyspark\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "056231f1-ccc4-4f31-8f21-297024cb6a91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Colab PySpark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03da75e8-b894-44ce-9bd9-91b7112df129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /tmp/stream_orders\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "\n",
    "# Define schema\n",
    "schema = StructType() \\\n",
    "    .add(\"order_id\", StringType()) \\\n",
    "    .add(\"customer_id\", StringType()) \\\n",
    "    .add(\"product\", StringType()) \\\n",
    "    .add(\"quantity\", IntegerType()) \\\n",
    "    .add(\"region\", StringType())\n",
    "\n",
    "# Sample data\n",
    "initial_data = [\n",
    "    (\"1\", \"C101\", \"Laptop\", 2, \"South\"),\n",
    "    (\"2\", \"C102\", \"Chair\", 6, \"North\"),\n",
    "    (\"3\", \"C103\", \"Mobile\", 1, \"East\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(initial_data, schema=schema)\n",
    "\n",
    "# Save as CSV in local path\n",
    "output_path = \"/tmp/stream_orders\"  # Local path in Colab VM\n",
    "\n",
    "df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"header\", True) \\\n",
    "  .csv(output_path)\n",
    "\n",
    "print(\"Saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c78867ae-a15e-463b-b8f9-4c0e3a3abac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .schema(schema)\n",
    "    .option(\"header\", True)\n",
    "    .csv(\"/tmp/stream_orders\")  # local path instead of dbfs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d112c9-8f11-47c6-826a-cbe657cc4f13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "transformed_orders = orders_stream.withColumn(\n",
    "    \"bulk_order\", when(col(\"quantity\") > 5, True).otherwise(False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dd803b8-ab2c-411e-8098-05e7a94b7c98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Create rate stream\n",
    "rate_df = (\n",
    "    spark.readStream\n",
    "    .format(\"rate\")\n",
    "    .option(\"rowsPerSecond\", 1)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Add is_even column\n",
    "transformed_df = rate_df.withColumn(\"is_even\", (col(\"value\") % 2 == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5bc7983-79b6-474a-a646-06c0ea9c9361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Write to memory table\n",
    "query = (\n",
    "    transformed_df.writeStream\n",
    "    .format(\"memory\")         # Write to in-memory table\n",
    "    .queryName(\"rate_table\")  # Query name to use with spark.sql()\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ff3a321-07d3-46c2-9093-9f5b31cabe76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Wait a few seconds for data to accumulate\n",
    "import time\n",
    "time.sleep(5)  # Allow some data to be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7895c1d8-1f3a-45ba-a41c-92bed60f4883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------+\n|           timestamp|value|is_even|\n+--------------------+-----+-------+\n|2025-08-11 09:12:...|    0|   true|\n|2025-08-11 09:12:...|    1|  false|\n|2025-08-11 09:12:...|    2|   true|\n|2025-08-11 09:12:...|    3|  false|\n|2025-08-11 09:12:...|    4|   true|\n|2025-08-11 09:12:...|    5|  false|\n|2025-08-11 09:12:...|    6|   true|\n|2025-08-11 09:12:...|    7|  false|\n|2025-08-11 09:12:...|    8|   true|\n|2025-08-11 09:12:...|    9|  false|\n|2025-08-11 09:12:...|   10|   true|\n|2025-08-11 09:12:...|   11|  false|\n|2025-08-11 09:12:...|   12|   true|\n|2025-08-11 09:12:...|   13|  false|\n|2025-08-11 09:12:...|   14|   true|\n|2025-08-11 09:12:...|   15|  false|\n|2025-08-11 09:12:...|   16|   true|\n|2025-08-11 09:12:...|   17|  false|\n|2025-08-11 09:12:...|   18|   true|\n|2025-08-11 09:12:...|   19|  false|\n+--------------------+-----+-------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Query in-memory table\n",
    "spark.sql(\"SELECT * FROM rate_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea9d79b9-cfa2-4582-84da-29c424e29b46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 7: Stop the query\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988db958-4147-40bb-8537-ebc22ba546f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+--------------+-------------+\n|           timestamp|value|multiple_of_5|multiple_of_10|size_category|\n+--------------------+-----+-------------+--------------+-------------+\n|2025-08-11 09:13:...|    0|         true|          true|        Small|\n|2025-08-11 09:13:...|    1|        false|         false|        Small|\n|2025-08-11 09:13:...|    2|        false|         false|        Small|\n|2025-08-11 09:13:...|    3|        false|         false|        Small|\n+--------------------+-----+-------------+--------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Doing some variations to this stream\n",
    "\n",
    "# To check the number is a multiple of 5 and 10\n",
    "# Categorize numbers as 'Small', 'Medium', or 'Large'\n",
    "\n",
    "# Transform the stream\n",
    "transformed_df = rate_df \\\n",
    "    .withColumn(\"multiple_of_5\", (col(\"value\") % 5 == 0)) \\\n",
    "    .withColumn(\"multiple_of_10\", (col(\"value\") % 10 == 0)) \\\n",
    "    .withColumn(\"size_category\",\n",
    "                when(col(\"value\") < 10, \"Small\")\n",
    "                .when(col(\"value\") < 50, \"Medium\")\n",
    "                .otherwise(\"Large\"))\n",
    "\n",
    "# Write stream to in-memory table\n",
    "query = (\n",
    "    transformed_df.writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"rate_table\")  # You can query this later\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Wait to accumulate data\n",
    "import time\n",
    "time.sleep(5)\n",
    "\n",
    "# Query and display the streaming memory table\n",
    "spark.sql(\"SELECT * FROM rate_table\").show()\n",
    "\n",
    "# Stop the stream after viewing\n",
    "query.stop()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc1c6eb3-4b5b-41db-b495-baed6d44c53d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efbac3ef-755f-4269-97c5-a8881eaa78df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fb082ba-2684-4493-b03c-d2fbc2e63678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "August 8 Practise",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}